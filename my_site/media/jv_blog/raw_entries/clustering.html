<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head><title>Clustering</title> 
<meta charset='utf-8' /> 
<meta name='generator' content='TeX4ht (https://tug.org/tex4ht/)' /> 
<meta name='viewport' content='width=device-width,initial-scale=1' /> 
<link href='clustering.css' rel='stylesheet' type='text/css' /> 
<meta name='src' content='clustering.tex' /> 
</head><body>
<div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'>Clustering</h2>
<div class='author'><span class='cmr-12'>John Vorsten</span></div>
<br />
<div class='date'><span class='cmr-12'>May 2020</span></div>
</div>
<h3 class='sectionHead'><span class='titlemark'>1   </span> <a id='x1-10001'></a>Introduction</h3>
<!-- l. 28 --><p class='noindent'><span class='paragraphHead'><a id='x1-20001'></a><span class='cmbx-10'>Clustering</span></span>
is breaking sets of objects into groups (or clusters) so that objects within groups are
similar to each other. A <span class='cmti-10'>clusterer </span>is a algorithm or function that is responsible or
grouping objects. It requires a <span class='cmti-10'>distance measurement </span>between objects and
and <span class='cmti-10'>aggregation function </span>to decide which group an object belongs in. The
aggregation function requires an assumption to be made about the data it operates
on [<span class='cite'><a href='#XCharrad2014'>Charrad et al.</a></span>, <span class='cite'><a href='#XCharrad2014'>2014a</a></span>]. Without the assumption it would not be able
to divide data into groups. Usually that assumption comes in the form of
</p>
     <ul class='itemize1'>
     <li class='itemize'>the desired number of clusters
     </li>
     <li class='itemize'>a density lower limit for objects in a cluster
     </li>
     <li class='itemize'>a maximum distance between objects to fit in a cluster
     </li>
     <li class='itemize'>some other form of distance limit or statistical metric</li></ul>
                                                                  

                                                                  
<!-- l. 37 --><p class='noindent'>So, partitioning a set of examples into clusters depends on the the assumption used
to calculate similarity within a cluster. If that assumption is not good, then the
aggregation won’t be useful. If the assumption is good, then the aggregation might
output useful groups.
</p><!-- l. 39 --><p class='noindent'>In practice, the assumption used to partition data (number of clusters, or density
threshold) is subjective. In a dataset, </p>
     <ul class='itemize1'>
     <li class='itemize'>Variance and density are affected by the size of the feature space
     </li>
     <li class='itemize'>Natural  clusters  could  overlap,  or  be  skewed  in  one  direction  (non
     spherical)
     </li>
     <li class='itemize'>Clusters might be abstract
     </li>
     <li class='itemize'>The number of clusters that minimize some variance metric might not be
     the number of clusters that the user expects</li></ul>
<!-- l. 47 --><p class='noindent'>This means a clustering scheme needs a way to validate its output, which also
validates the assumption. The process of evaluating results of a clusterer is known as
<span class='cmti-10'>cluster validity </span>[<span class='cite'><a href='#XCharrad2014'>Charrad et al.</a></span>, <span class='cite'><a href='#XCharrad2014'>2014a</a></span>].
</p><!-- l. 49 --><p class='noindent'><span class='cite'><a href='#XPattern_Recognition'>Pat</a></span> and <span class='cite'><a href='#XCharrad2014'>Charrad et al.</a></span> [<span class='cite'><a href='#XCharrad2014'>2014a</a></span>] mention a few ways to measure cluster validity (find
the most optimal number of clusters or density assumption). This project will
determine the optimal number of clusters based on how a cluster validity metric
changes as the <span class='cmti-10'>number of clusters </span>assumption is varied.
</p><!-- l. 51 --><p class='noindent'>This article will cover choosing <span class='cmti-10'>clusterers</span>, including its <span class='cmti-10'>distance measurements,
</span><span class='cmti-10'>aggregation functions</span>, and <span class='cmti-10'>metrics </span>used to validate the output number of
clusters on a unique dataset. By the end, I hope to have a framework that will
automatically find the best number of clusters in my dataset, cluster it into
groups, and maybe even predict the best clusterer to use based on the data
characteristics.
</p><!-- l. 53 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>2   </span> <a id='x1-30002'></a>Problem Statement</h3>
<!-- l. 54 --><p class='noindent'>Deciding on the number of clusters that should fit a dataset is hard. Imagine if the
data has more than 3 dimensions - it is impossible to visualize without losing
dimensionality. Estimating a benchmark cluster density is hard (especially in high
dimensional spaces). It gets worse if multiple datasets need to be clustered and the
data’s density distribution varies across datasets. Removing human input and
                                                                  

                                                                  
automating the ‘number of clusters‘ decision, again, adds complexity. This blog refers
to this type of clustering as (without a human inputting the assumption)
<span class='cmti-10'>unsupervised clustering</span>.
</p><!-- l. 56 --><p class='noindent'>This project’s goal is to optimally choose the best number of clusters that fit a
dataset. Later, the clustered examples will be fed into a ranking/multi-instance
classification machine for bag classification. The un-clustered input is a large amount
of examples with between two to several hundred examples per set. The examples
comes from databases of building automation controllers, and each example has
many features. The examples within a dataset can naturally be divided into groups
by a trained human (more below). Hopefully, a combination of an aggregation
function and distance metric can be found that optimally partition datasets without
human input.
</p><!-- l. 59 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>3   </span> <a id='x1-40003'></a>Data Description</h3>
<!-- l. 61 --><p class='noindent'>Features include <span class='cmti-10'>NAME, TYPE, SLOPE, [...]</span>. Figure <a href='#x1-4001r1'>1<!-- tex4ht:ref: fig:data_features  --></a> shows typical features of a
dataset : </p><figure class='figure'> 

                                                                  

                                                                  
<a id='x1-4001r1'></a>
                                                                  

                                                                  
<!-- l. 65 --><p class='noindent'><img src='https://images2.imgbox.com/06/45/eL6dWg0j_o.png' alt='PIC' />
<a id='x1-4002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 1:</span><span class='content'>Example objects with features</span></figcaption><!-- tex4ht:label?: x1-4001r3  -->
                                                                  

                                                                  
</figure>
<!-- l. 70 --><p class='noindent'>By observing the name feature, a trained human could identify the number of
clusters. The name features are organized hierarchically, with most points sharing a
common suffix (samc), and lower abbreviations can represent location, equipment
type, and device type. Generally, all instances that belong under one cluster (or
‘system’) are named similarly except for the ending tag. For example, in Figure <a href='#x1-4001r1'>1<!-- tex4ht:ref: fig:data_features  --></a> are
(4) different systems, and each system share similar name feature suffixes except for
the ending tag.
</p>
<h3 class='sectionHead'><span class='titlemark'>4   </span> <a id='x1-50004'></a>Data Exploration</h3>
<!-- l. 73 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>4.1   </span> <a id='x1-60004.1'></a>Extract</h4>
<!-- l. 74 --><p class='noindent'>Extracting information from existing databases is straight forward. There were many
SQL databases located on remote servers. From each database, a few tables were
selected which hold the bulk of useful features for this project. Extracting data
included : </p>
     <ul class='itemize1'>
     <li class='itemize'>Copying database files from remote servers to a local hard disc
     </li>
     <li class='itemize'>Create new table schemas for a new master database
     </li>
     <li class='itemize'>Iterate through copied database files and copy table data to new master
     database</li></ul>
<!-- l. 80 --><p class='noindent'>The new database is used as feeder to the Transform process stage.
</p><!-- l. 83 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>4.2   </span> <a id='x1-70004.2'></a>Transform</h4>
<!-- l. 84 --><p class='noindent'>There are many features to choose from the available data. This project focuses on
clustering based on the ‘NAME‘ attribute. Each instance’s name is a text feature.
The name is organized hierarchically delimited by non-alphanumeric characters
(’.’,’-’). This section describes transforming the name feature into data that can be
                                                                  

                                                                  
clustered.
</p>
<h4 class='subsectionHead'><span class='titlemark'>4.3   </span> <a id='x1-80004.3'></a>Encoding</h4>
<!-- l. 86 --><p class='noindent'>I transformed the name feature into a one-hot encoded set of text features :
</p>
     <ul class='itemize1'>
     <li class='itemize'>split feature along delimiter
     </li>
     <li class='itemize'>construct a vocabulary
     </li>
     <li class='itemize'>transform to one-hot encoded feature column</li></ul>
<!-- l. 93 --><p class='noindent'>The result is a sparse feature space. All names in the dataset vocabulary are
included to capture all variance in the data; a pooled feature is not useful for
clustering.
</p><!-- l. 95 --><p class='noindent'>One-hot encoding has implications on the clustering algorithm aggregator
and distance metric. First, a vector will (depending on objects) vary by
2 directions in its space compared to a vector in the same system. This
means the Euclidean distance between two vectors of the same system will be
<span class='cmmi-10'>norm</span>(<span class='cmmi-10'>u </span><span class='cmsy-10'>− </span><span class='cmmi-10'>v</span>) = [(<span class='cmmi-10'>u</span><sub><span class='cmr-7'>1</span></sub> <span class='cmsy-10'>− </span><span class='cmmi-10'>v</span><sub><span class='cmr-7'>1</span></sub>)<sup><span class='cmr-7'>2</span></sup> + [<span class='cmmi-10'>...</span>] + (<span class='cmmi-10'>u</span><sub><span class='cmmi-7'>n</span></sub> <span class='cmsy-10'>− </span><span class='cmmi-10'>v</span><sub><span class='cmmi-7'>n</span></sub>)<sup><span class='cmr-7'>2</span></sup>]<sup><span class='cmr-7'>1</span><span class='cmmi-7'>∕</span><span class='cmr-7'>2</span></sup>. Also, Euclidean distance will
increase following <img class='sqrt' src='clustering0x.png' alt='√-
 2' /><span class='cmmi-10'>,</span><img class='sqrt' src='clustering1x.png' alt='√-
 4' /><span class='cmmi-10'>,</span>[<span class='cmmi-10'>...</span>].
</p><!-- l. 97 --><p class='noindent'>Instances of a similar system should have features distributed “spherically” in their
feature space. The clustering algorithm will have to recognize objects with high
density in a similar feature space.
</p><!-- l. 99 --><p class='noindent'>An object’s distance in space can be visualized nicely using multidimensional scaling
(MDS), which is a dimensionality reduction technique that tries to maintain
distances between features. Each systems cluster is mostly isolated from others, and
each instance within a cluster is separated from others. Figure 2 represents one
database of points that are transformed and reduced to (2) dimensions. There are (3)
clearly defined clusters with a large amount of instances, and (2) small clusters with
(2) and (3) instances each.
</p>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-8001r2'></a>
                                                                  

                                                                 <!-- MDS_Reduction -->
<!-- l. 104 --><p class='noindent'><img src='https://images2.imgbox.com/8f/54/zXPTOzEN_o.png' alt='PIC' />
<a id='x1-8002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 2:</span><span class='content'>Dimensionality-reduction is a good way to visualize distributions of
data across a large instance space</span></figcaption><!-- tex4ht:label?: x1-8001r4  -->
                                                                  

                                                                  
</figure>
<h4 class='subsectionHead'><span class='titlemark'>4.4   </span> <a id='x1-90004.4'></a>Load</h4>
<!-- l. 110 --><p class='noindent'>Raw data was loaded from a SQL Server, transformed, and fed to the next step.
Transforming raw data was quick, and the transformed features easily fit in memory.
Pipelines were tweaked to meet the clustering goal.
</p><!-- l. 113 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>5   </span> <a id='x1-100005'></a>Clustering - Aggregation Functions and Evaluation Metrics</h3>
<!-- l. 114 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>5.1   </span> <a id='x1-110005.1'></a>Aggregation Functions</h4>
<!-- l. 115 --><p class='noindent'>This project uses two classes of aggregation functions : </p>
     <ul class='itemize1'>
     <li class='itemize'>Hierarchical
     </li>
     <li class='itemize'>K-means</li></ul>
<!-- l. 120 --><p class='noindent'>K-means is a general-purpose aggregation function, and is the first I found
when learning about clustering. K-means attempts to separate examples into
<span class='cmti-10'>n-groups </span>by minimizing the sum of squares between centroids and points
assigned to the centroid. It is an iterative process which alternates between
</p>
     <ul class='itemize1'>
     <li class='itemize'>first assigning points to a centroid,
     </li>
     <li class='itemize'>and then shifting the centroid to minimize the sum of squares criteria
     [<span class='cite'><a href='#XVLFeat'>Authors</a></span>, <span class='cite'><a href='#XVLFeat'>2007</a></span>]</li></ul>
<!-- l. 126 --><p class='noindent'>Hierarchial or agglomerative clustering is the process of successively splitting or
merging clusters based on a <span class='cmti-10'>linkage </span>criteria between clusters. This paper used <span class='cmti-10'>ward</span>
and <span class='cmti-10'>average </span>linkage. Ward linkage criteria seek to minimize sum of squared
differences within clusters, and average linkage minimizes average distance within
clusters. Both methods are similar to K-means because they try to minimize variance
within clusters.
                                                                  

                                                                  
</p><!-- l. 128 --><p class='noindent'>Both average and ward linkage work well on spherical, data. My data is globular, but
overlapping. Ideally I would use a linkage that works well with non-globular data,
and also use a threshold restriction that prevents overlapping examples from merging
to the same cluster.
</p><!-- l. 130 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>5.2   </span> <a id='x1-120005.2'></a>Evaluation Metrics</h4>
<!-- l. 131 --><p class='noindent'>Evaluation metrics are used to quantify the quality of clusters. They help us
understand how well the clustering scheme fits a dataset. Evaluation metrics can also
help us choose the optimal number of partitions to feed to the aggregation
function. In this way an evaluation metric helps us solve the <span class='cmti-10'>cluster validity</span>
problem.
</p><!-- l. 133 --><p class='noindent'>Each evaluation metric included in the NbClust software package include some
measure of compactness and isolation of the data [<span class='cite'><a href='#XCharrad2014'>Charrad et al.</a></span>, <span class='cite'><a href='#XCharrad2014'>2014a</a></span>]. By varying
the input clustering assumption (number of clusters) and calculating evaluation
metrics on the resulting clusters, we can see how well the ‘number of clusters‘
assumption holds. In this way finding the optimal number of clusters is very similar
to finding the optimal hyperparameters for some regression or prediction
model.
</p><!-- l. 135 --><p class='noindent'>To illustrate this, Figure <a href='#x1-12003r4'>4<!-- tex4ht:ref: fig:cindex  --></a> shows the <span class='cmti-10'>C Index </span>versus number of clusters assumption
for test data. The left graph shows raw data, and the right graph shows C Index. The
optimal number of clusters is the value which minimizes the C Index. The C Index is
minimal at 3 or 5 clusters depending on how the C Index is implemented. It is
ambiguous whether 3 or 5 is the correct number of clusters, although the data was
generated with 5 clusters.
</p>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-12001r3'></a>
                                                                  

<!-- images/test-data.png          -->
<!-- l. 156 --><p class='noindent'><img src='https://images2.imgbox.com/fa/34/hHHoq2Ki_o.png' alt='PIC' />
<a id='x1-12002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 3:</span><span class='content'>Sample data clusters</span></figcaption><!-- tex4ht:label?: x1-12001r5  -->
                                                                  

                                                                  
</figure>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-12003r4'></a>
                                                                  

                                                          <!-- c-index.png         -->
<!-- l. 163 --><p class='noindent'><img src='https://images2.imgbox.com/32/a4/gGpA7DYI_o.png' alt='PIC' />
<a id='x1-12004'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 4:</span><span class='content'>C Index calculated on sample data</span></figcaption><!-- tex4ht:label?: x1-12003r5  -->
                                                                  

                                                                  
</figure>
<h3 class='sectionHead'><span class='titlemark'>6   </span> <a id='x1-130006'></a>Ranking introduction</h3>
<!-- l. 170 --><p class='noindent'>In ranking, the goal is to order a list of examples based on the relevance of each
example to a question. In this project, examples are clustering schemes, and the
question is a dataset. A quality ranking model will optimally sort a list of clustering
schemes, such that the clustering scheme that is most likely to predict the desired
number of clusters is recommended. Ranking has applications in document
retrieval and question answering, but the format works well for this project
goal.
</p><!-- l. 172 --><p class='noindent'>Formally, let <span class='cmbx-10'>X</span> represent the universe of items, and <span class='cmbx-10'>x </span><span class='cmsy-10'>∈ </span><span class='cmbx-10'>X</span><sup><span class='cmbx-7'>n</span></sup> represents a list of <span class='cmti-10'>n</span>
items, with <span class='cmmi-10'>x</span><sub><span class='cmmi-7'>i</span></sub> <span class='cmsy-10'>∈ </span><span class='cmbx-10'>x </span>the ith element in <span class='cmbx-10'>x</span>. Π is a permutation of all items in the list <span class='cmbx-10'>x</span>.
The ranking function <span class='cmmi-10'>f </span>: <span class='cmbx-10'>X</span><sup><span class='cmbx-7'>n</span></sup> <span class='cmsy-10'>→ </span>Π<sup><span class='cmmi-7'>n</span></sup> produces a permutation of the list that maximizes
the list’s utility.
</p><!-- l. 174 --><p class='noindent'>A clustering scheme is a combination of aggregation function and validity
index. Many combinations of aggregation functions and validity indices form
<span class='cmbx-10'>x </span><span class='cmsy-10'>∈ </span><span class='cmbx-10'>X</span><sup><span class='cmbx-7'>n</span></sup>. The ranking of <span class='cmbx-10'>x </span>produces Π which is an ordering which outputs the
clustering scheme most likely to predict the correct number of clusters for a
dataset.
</p><!-- l. 176 --><p class='noindent'>The training data of <span class='cmti-10'>m </span>elements is defined as <span class='cmmi-10'>L</span><sup><span class='cmmi-7'>m</span></sup> = <span class='cmsy-10'>{</span>(<span class='cmbx-10'>x</span><span class='cmmi-10'>,π</span>)<span class='cmsy-10'>|</span><span class='cmbx-10'>x </span><span class='cmsy-10'>∈ </span><span class='cmbx-10'>X</span><sup><span class='cmbx-7'>n</span></sup><span class='cmmi-10'>,π </span><span class='cmsy-10'>∈ </span>Π<sup><span class='cmmi-7'>n</span></sup>)<span class='cmsy-10'>}</span>. Each
<span class='cmmi-10'>x</span><sub><span class='cmmi-7'>i</span></sub> <span class='cmsy-10'>∈ </span><span class='cmbx-10'>x </span>is really a pair of (database features, clustering scheme), where database
features are <span class='cmti-10'>context features</span>, and clustering schemes are <span class='cmti-10'>per-item features</span>. Each
<span class='cmmi-10'>x</span><sub><span class='cmmi-7'>i</span></sub> <span class='cmsy-10'>∈ </span><span class='cmbx-10'>x </span>is therefore a feature vector which includes database features and clustering
schemes, and each clustering scheme is given a relative ranking to how well it predicts
on the database.
</p><!-- l. 178 --><p class='noindent'>This project uses TF-Ranking <span class='cite'><a href='#XTF-Ranking'>Pasumarthi et al.</a></span> [<span class='cite'><a href='#XTF-Ranking'>2019</a></span>], which is a open source
Tensorflow deep neural network learning framework. Other ranking functions
could be used, including linear, gradient trees, and support vector machines,
but this project only tested a neural network as the scoring function [<span class='cite'><a href='#Xgroupwise_scoring'>Ai
et al.</a></span>, <span class='cite'><a href='#Xgroupwise_scoring'>2019</a></span>].
</p><!-- l. 180 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>6.1   </span> <a id='x1-140006.1'></a>Metrics</h4>
<!-- l. 181 --><p class='noindent'>Normalized discounted cumulative gain (NDCG)is the ranking metric used to
measure goodness of fit. NDCG measures the cumulative gain of a ranking. Relevant
items ranked early in the list add gain, and relevant items ranked last in the list are
penalized.
                                                                  

                                                                  
</p><!-- l. 184 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>6.2   </span> <a id='x1-150006.2'></a>Training Data features - Context</h4>
<!-- l. 185 --><p class='noindent'>There are challenges to choosing an optimal number of clusters for this
problem.
</p><!-- l. 187 --><p class='noindent'>First, the number of features in each dataset varies between ten to a thousand. This
means the closeness and compactness of clusters changes between datasets.
Because each evaluation metric measures some form of geometric or statistical
properties, a metric might perform well on a small dataset and poorly on large
datasets.
</p><!-- l. 189 --><p class='noindent'>Second, each datasets correct number of clusters varies. How well does a metric
perform when the ratio of objects to clusters changes?
</p><!-- l. 191 --><p class='noindent'>Last, the dimensionality of an instances feature space changes between datasets.
Again, dimensionality affects the relative density of objects within a space.
</p><!-- l. 193 --><p class='noindent'>This project looks for a relationship between accuracy of validation indices and the
features of a dataset that define the datasets geometric properties. For this
experiment, the following features of each dataset are used :
</p><!-- l. 195 --><p class='noindent'>
     </p><ol class='enumerate1'>
     <li class='enumerate' id='x1-15002x1'>Number of examples or objects
     </li>
     <li class='enumerate' id='x1-15004x2'>Feature space dimensionality
     </li>
     <li class='enumerate' id='x1-15006x3'>Variance of word lengths
     </li>
     <li class='enumerate' id='x1-15008x4'>Ratio of number of examples to number of features
     </li>
     <li class='enumerate' id='x1-15010x5'>Number of NAME word tokens 1 token long, 2 tokens long ... to ¿ 5 tokens
     long</li></ol>
<!-- l. 204 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>6.3   </span> <a id='x1-160006.3'></a>Training Data Features - Per Item</h4>
                                                                  

                                                                  
<!-- l. 205 --><p class='noindent'>Per-item features include information about a) how a dataset is clustered and b) how
the optimal number of clusters is predicted. A dataset is clustered with a
<span class='cmti-10'>clustering algorithm</span>, and the number of clusters is predicted with a <span class='cmti-10'>validity
</span><span class='cmti-10'>index</span>.
</p><!-- l. 207 --><p class='noindent'>In addition, dimensionality reduction was alternately applied before clustering data.
The full set of per-item features includes the categorical features (Clusterer, index,
dimensionality reduction). The full set of categorical features is shown in Appendix
<a href='#x1-26000A'>A<!-- tex4ht:ref: appendix:A  --></a>.
</p><!-- l. 210 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>6.4   </span> <a id='x1-170006.4'></a>Data Labeling &amp; NbClust</h4>
<!-- l. 211 --><p class='noindent'>Each dataset was clustered using software packages NbClust [<span class='cite'><a href='#XNbClust'>Charrad et al.</a></span>, <span class='cite'><a href='#XNbClust'>2014b</a></span>]
and Scikit-Learn [<span class='cite'><a href='#Xscikit-learn'>Pedregosa et al.</a></span>, <span class='cite'><a href='#Xscikit-learn'>2011</a></span>]. Then, the predicted number of clusters
was calculated from evaluation metrics. The label for the (context-feature, per-item
feature) pair is the accuracy of the predicted number of clusters to the actual number
of clusters. See Figure <a href='#x1-17001r5'>5<!-- tex4ht:ref: ranking_features  --></a> for an illustration of per-item features, context features, and
labels.
</p>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-17001r5'></a>
                                                                  

<!-- images/feature_visualization.png -->
<!-- l. 221 --><p class='noindent'>                               <img src='https://images2.imgbox.com/1b/73/7k1UOrwv_o.png' alt='PIC' />
<a id='x1-17002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 5:</span><span class='content'>Ranking  Per-item  features  (clustering  parameters)  and  context
features (features of the dataset)</span></figcaption><!-- tex4ht:label?: x1-17001r6  -->
                                                                  

                                                                  
</figure>
<!-- l. 226 --><p class='noindent'>Before training, labels were binned into (5) categories. Labels with a value of (5)
indicated the most accurate clustering scheme, and labels of (1) indicated the least
accurate clustering scheme. There are up to 200 labels per (Context Features,
Peritem Features) pair. This means there are up to 200 <span class='cmsy-10'>÷ </span>5 = 40 labels with value
(5), 40 labels with value (4), etc.
</p>
<h4 class='subsectionHead'><span class='titlemark'>6.5   </span> <a id='x1-180006.5'></a>Ranking Results</h4>
<!-- l. 230 --><p class='noindent'>The ranking scoring model was trained, and NDCG results were recorded
(Figure <a href='#x1-18001r6'>6<!-- tex4ht:ref: fig:ndcg5  --></a>). The ranking model showed a small improvement in gain. When
measuring NDCG of the top 5 ranked clustering schemes the ranking model is
able to improve gain from a minimum of <span class='cmti-10'>0.67 </span>to a plateau of <span class='cmti-10'>0.75 </span>(Figure
<a href='#x1-18001r6'>6<!-- tex4ht:ref: fig:ndcg5  --></a>).
</p><!-- l. 232 --><p class='noindent'>Notice how NDCG improves less when measuring the top 25 items in the ranked list
(Figure <a href='#x1-18003r7'>7<!-- tex4ht:ref: fig:ndcg25  --></a>). This indicates there is more ambiguity in the relevance of clustering
schemes lower in the list.
</p>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-18001r6'></a>
                                                                  

<!-- images/NDCG_5_model4.png -->
<!-- l. 252 --><p class='noindent'>                               <img src='https://images2.imgbox.com/1f/b9/OsQuOZHF_o.png' alt='PIC' />
<a id='x1-18002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 6:</span><span class='content'>NDCG of top 5 clustering schemes in list</span></figcaption><!-- tex4ht:label?: x1-18001r6  -->
                                                                  

                                                                  
</figure>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-18003r7'></a>
                                                                  

<!-- images/NDCG_25_model4.png -->
<!-- l. 258 --><p class='noindent'>                               <img src='https://images2.imgbox.com/44/34/kR4G1hlz_o.png' alt='PIC' />
<a id='x1-18004'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 7:</span><span class='content'>NDCG of top 25 clustering schemes in list</span></figcaption><!-- tex4ht:label?: x1-18003r6  -->
                                                                  

                                                                  
</figure>
<h3 class='sectionHead'><span class='titlemark'>7   </span> <a id='x1-190007'></a>Conclusion and Improvements</h3>
<!-- l. 265 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>7.1   </span> <a id='x1-200007.1'></a>Conclusion</h4>
<!-- l. 266 --><p class='noindent'>While the ranking model did improve NDCG modestly, I do not believe the model
found a significant relationship between the database features (context features) and
clustering schemes (peritem features).
</p><!-- l. 268 --><p class='noindent'>I hypothesize that the ranking model recognized consistently poorly performing
validity indices and learned to rank them low. Conversely, the model might
recognize validity indices that consistently perform better, and rank those
high.
</p><!-- l. 270 --><p class='noindent'>
</p>
<h4 class='subsectionHead'><span class='titlemark'>7.2   </span> <a id='x1-210007.2'></a>Improvements</h4>
<!-- l. 271 --><p class='noindent'>There are a lot of opportunities for improvements in this project, including :
</p>
     <ul class='itemize1'>
     <li class='itemize'>Use different aggregation functions that perform better with unbalanced
     datasets
     </li>
     <li class='itemize'>Add connectivity constraints to aggregation functions
     </li>
     <li class='itemize'>Use a regression or DNN model to predict the best number of clusters
     based on validity index results directly</li></ul>
<!-- l. 278 --><p class='noindent'><span class='paragraphHead'><a id='x1-220007.2'></a><span class='cmbx-10'>Aggregation Functions</span></span>
have personalities and perform slightly differently from each other. Kmeans performs
well on evenly spaced data with few clusters, while Agglomerative aggregation
performs well with many clusters. Data characteristics to consider include a) number
                                                                  

                                                                  
of clusters, b) even cluster sizes, c) cluster geometry (manifolded / flat), d) outlier
presence (See Figure <a href='#x1-23003r9'>9<!-- tex4ht:ref: fig:constraints_and_kmeans  --></a>).
</p><!-- l. 281 --><p class='noindent'>This projects aggregation functions only included Kmeans and Agglomerative, but
others like Affinity Propagation, Mean-shift, DBSCAN, or birch may perform
better.
</p>
<!-- l. 283 --><p class='noindent'><span class='paragraphHead'><a id='x1-230007.2'></a><span class='cmbx-10'>Connectivity constraints</span></span>
are distance thresholds that prevent clusters from being merged that are not
adjacent. Because this projects text data is highly structured and hierarchical,
connectivity constraints could give a better estimate of cluster boundaries than a
density metric. A connectivity constraint could be made to disallow clustering of text
features that are not directly adjacent when the text is sorted alphabetically (Figure
<a href='#x1-23003r9'>9<!-- tex4ht:ref: fig:constraints_and_kmeans  --></a>).
</p><!-- l. 286 --><p class='noindent'>I believe that most of this projects data could be better clustered with carefully
crafted connectivity constraints instead of density estimates.
</p>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-23001r8'></a>
                                                                  

<!-- images/kmeans_circles.png                   -->
<!-- l. 305 --><p class='noindent'><img src='https://images2.imgbox.com/db/ec/avpl0YZ4_o.png' alt='PIC' />
<a id='x1-23002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 8:</span><span class='content'>Kmeans clustering on non-flat data</span></figcaption><!-- tex4ht:label?: x1-23001r7  -->
                                                                  

                                                                  
</figure>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-23003r9'></a>
                                                                  

<!-- images/connectivity_constraint.png -->
<!-- l. 311 --><p class='noindent'><img src='https://images2.imgbox.com/bf/60/6XQOzMpH_o.png' alt='PIC' />
<a id='x1-23004'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 9:</span><span class='content'>Connectivity Constraint</span></figcaption><!-- tex4ht:label?: x1-23003r7  -->
                                                                  

                                                                  
</figure>
<!-- l. 318 --><p class='noindent'><span class='paragraphHead'><a id='x1-240007.2'></a><span class='cmbx-10'>Direct inference with DNN / regression</span></span>
When the ’optimal’ number of clusters is chosen from a validity index, the optimal
number of clusters occurs at some function of the validity index. Generally, if <span class='cmbx-10'>X </span>is
the validity index vector, where <span class='cmmi-10'>x</span><sub><span class='cmmi-7'>i</span></sub> <span class='cmsy-10'>∈ </span><span class='cmbx-10'>X </span>is the ith validity measurement, the goal is to
find a function <span class='cmmi-10'>f </span>: <span class='cmbx-10'>X </span><span class='cmsy-10'>→ </span><span class='msbm-10'>ℤ </span>where <span class='msbm-10'>ℤ </span>is the set of all real integers. The function <span class='cmmi-10'>f</span>
should maximize or minimize some sort of criteria. Because most validity indices
measure cluster density or between-cluster spacing, it is natural for the function <span class='cmmi-10'>f </span>to
maximize cluster density or minimize entropy (Figure <a href='#x1-24001r10'>10<!-- tex4ht:ref: fig:gap_statistic  --></a>). Sometimes the function <span class='cmmi-10'>f</span>
is the function that maximizes the second-difference (usually called hte ’knee’) of
measurements.
</p><!-- l. 321 --><p class='noindent'>I wonder if applying a general DNN or sequential classifier could infer the correct
number of clusters better than a simple min/max. This problem could be treated
similar to the multiclass classification problem, where the input vector <span class='cmbx-10'>X </span>is the
validity index vector, and the correct number of clusters is the index of the maximum
softmax activated logit (Figure <a href='#x1-24003r11'>11<!-- tex4ht:ref: fig:dnn_number_clusters  --></a>).
</p>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-24001r10'></a>
                                                                  

<!-- images/gap_statistic.png -->
<!-- l. 341 --><p class='noindent'><img height='345' width='345' src='https://images2.imgbox.com/58/d1/gFGEABwT_o.png' alt='PIC' />
<a id='x1-24002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 10:</span><span class='content'>Gap statistic versus number of clusters</span></figcaption><!-- tex4ht:label?: x1-24001r7  -->
                                                                  

                                                                  
</figure>
<figure class='figure'> 

                                                                  

                                                                  
<a id='x1-24003r11'></a>
                                                                  

<!-- images/dnn_number_clusters.png -->
<!-- l. 349 --><p class='noindent'><img height='345' width='345' src='https://images2.imgbox.com/13/55/57SmXjon_o.png' alt='PIC' />
<a id='x1-24004'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 11:</span><span class='content'>Proposed DNN predicting optimal number of clusters</span></figcaption><!-- tex4ht:label?: x1-24003r7  -->
                                                                  

                                                                  
</figure>
<h3 class='likesectionHead'><a id='x1-250007.2'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XPattern_Recognition'></a><span class='bibsp'>   </span></span><span class='cmti-10'>Pattern Recognition</span>. Academic Press, 4 edition.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='Xgroupwise_scoring'></a><span class='bibsp'>   </span></span>Q. Ai,                                                                             X. Wang,
  S. Bruch, N. Golbandi, M. Bendersky, and M. Najork. Learning groupwise
  multivariate scoring functions using deep neural networks. In <span class='cmti-10'>Proceedings of
  </span><span class='cmti-10'>the 2019 ACM SIGIR International Conference on Theory of Information
  </span><span class='cmti-10'>Retrieval</span>, ICTIR ’19, page 85–92, New York, NY, USA, 2019. Association for
  Computing Machinery. ISBN 9781450368810. doi: 10.1145/3341981.3344218.
  URL <span class='cmtt-10'>https://doi.org/10.1145/3341981.3344218</span>.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XVLFeat'></a><span class='bibsp'>   </span></span>T. V.    Authors.            <span class='cmti-10'>K-means    fundamentals</span>,     2007.            URL
  <span class='cmtt-10'>https://www.vlfeat.org/api/kmeans-fundamentals.html</span>.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XCharrad2014'></a><span class='bibsp'>   </span></span>M. Charrad, N. Ghazzali, V. Boiteau, and A. Niknafs.   Nbclust : An r
  package for determining the relevant number of clusters in a data set. <span class='cmti-10'>Journal
  </span><span class='cmti-10'>of Statistical Software</span>, 61(6), 2014a.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XNbClust'></a><span class='bibsp'>   </span></span>M. Charrad,   N. Ghazzali,   V. Boiteau,   and   A. Niknafs.      NbClust:
  An  R  package  for  determining  the  relevant  number  of  clusters  in  a
  data  set.     <span class='cmti-10'>Journal  of  Statistical  Software</span>,  61(6):1–36,  2014b.     URL
  <span class='cmtt-10'>http://www.jstatsoft.org/v61/i06/</span>.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XTF-Ranking'></a><span class='bibsp'>   </span></span>R. K. Pasumarthi, S. Bruch, X. Wang, C. Li, M. Bendersky, M. Najork,
  J. Pfeifer,  N. Golbandi,  R. Anil,  and  S. Wolf.     Tf-ranking:  Scalable
  tensorflow library for learningto-rank. <span class='cmti-10'>Proceedings of the 25th ACM SIGKDD
  </span><span class='cmti-10'>International  Conference  on  Knowledge  Discovery  &amp;  Data  Mining</span>,  pages
  2970–2978, 2019. doi: 10.1145/3292500.3330677.
                                                                  

                                                                  
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='Xscikit-learn'></a><span class='bibsp'>   </span></span>F. Pedregosa,          G. Varoquaux,          A. Gramfort,          V. Michel,
  B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
  J. Vanderplas,  A. Passos,  D. Cournapeau,  M. Brucher,  M. Perrot,  and
  E. Duchesnay. Scikit-learn: Machine learning in Python. <span class='cmti-10'>Journal of Machine
  </span><span class='cmti-10'>Learning Research</span>, 12:2825–2830, 2011.
</p>
  </div>
<!-- l. 358 --><p class='noindent'>
</p>
<h3 class='sectionHead'><span class='titlemark'>A   </span> <a id='x1-26000A'></a>Per-item and Context Features</h3>
<!-- l. 360 --><p class='noindent'>All clustering indices used in this project
</p><!-- l. 362 --><p class='noindent'>[KL, CH, Hartigan, CCC, Marriot, TrCovW, TraceW, Friedman, Rubin, Cindex,
DB, Silhouette, Duda, PseudoT2, Beale, Ratkowsky, Ball, PtBiserial, Frey,
McClain, Dunn, Hubert, SDindex, Dindex, SDbw, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>gap_tib</span></span></span>, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>gap_star</span></span></span>, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>gap_max</span></span></span>,
Scott]
</p><!-- l. 364 --><p class='noindent'>Clustering algorithms used in project
</p><!-- l. 366 --><p class='noindent'>[average, kmeans, ward.D, Ward.D2]
</p>
 
</body> 
</html>